import torch
import torch.nn as nn
import math
from torch.autograd import Variable
from torch.autograd import Function
import torch.nn.functional as F
from datetime import datetime
import copy

import numpy as np
import utils_own

'''
def quantize(number,bitwidth):
    temp=1/bitwidth
    if number>0:
        for i in range(1,bitwidth):
            if number<=temp*i:
                return 2*i-1
        return 2*bitwidth-1
    else:
        for i in range(1,bitwidth):
            if number>=-temp*i:
                return -(2*i-1)
        return -(2*bitwidth-1)
'''


def Binarize(tensor, quant_mode='det', bitwidth=2):
   if quant_mode == 'weight':
      # temp = torch.floor(tensor.div_(2)).mul_(2).add_(1).mul_(tensor).div_(tensor)
      temp = torch.floor(tensor.mul_(2 ** bitwidth).div_(2)).mul_(2).add_(1).mul_(tensor).div_(tensor).div_(
         2 ** bitwidth)
      temp[temp != temp] = 0
      return temp
   if quant_mode == 'input':
      #return torch.round(tensor.mul_(45))  # tensor.mul_(45).div_(128)
      return torch.clamp(tensor.mul_(45).div_(128),min=-0.99,max=0.99)
   if quant_mode == 'multi':
      # result=[]
      # fraction_part =  tensor
      # fraction_part = torch.round(fraction_part * (2 ** (bitwidth)))
      # fraction_part = fraction_part / (2 ** bitwidth)
      # for i in range(1,bitwidth+1):
      #    result.append((fraction_part > 0).float() * (1 / (2 ** i)) * 2 - (1 / (2 ** i)))
      #    fraction_part -= result[-1]
      # return tensor.sign()
      # temp = torch.floor(tensor.div_(2)).mul_(2).add_(1).mul_(tensor).div_(tensor)
      # result=[tensor.clone() for _ in range(bitwidth)]
      # for i in range(1,bitwidth+1):
      temp = torch.floor(tensor.mul_(2 ** bitwidth).div_(2)).mul_(2).add_(1).mul_(tensor).div_(tensor).div_(
         2 ** bitwidth)
      temp[temp != temp] = 0
      return temp

   if quant_mode == 'det':
      return tensor.sign()
   if quant_mode == 'bin':
      return (tensor >= 0).type(type(tensor)) * 2 - 1
   else:
      return tensor.add_(1).div_(2).add_(torch.rand(tensor.size()).add(-0.5)).clamp_(0, 1).round().mul_(2).add_(-1)


def Ternarize(tensor, mult=0.7, mask=None, permute_list=None, pruned=False, align=False, pack=32):
   if type(mask) == type(None):
      mask = torch.ones_like(tensor)

   # Fix permutation. Tensor needs to be permuted
   if not pruned:
      tensor_masked = utils_own.permute_from_list(tensor, permute_list)
      if len(tensor_masked.size()) == 4:
         tensor_masked = tensor_masked.permute(0, 2, 3, 1)

      if not align:
         tensor_flat = torch.abs(tensor_masked.contiguous().view(-1)).contiguous()
         tensor_split = torch.split(tensor_flat, pack, dim=0)
         tensor_split = torch.stack(tensor_split, dim=0)
         tensor_sum = torch.sum(tensor_split, dim=1)
         tensor_size = tensor_sum.size(0)
         tensor_sorted, _ = torch.sort(tensor_sum)
         thres = tensor_sorted[int(mult * tensor_size)]
         tensor_flag = torch.ones_like(tensor_sum)
         tensor_flag[tensor_sum.ge(-thres) * tensor_sum.le(thres)] = 0
         tensor_flag = tensor_flag.repeat(pack).reshape(pack, -1).transpose(1, 0).reshape_as(tensor_masked)

      else:
         tensor_flat = torch.abs(tensor_masked.reshape(tensor_masked.size(0), -1)).contiguous()
         tensor_split = torch.split(tensor_flat, pack, dim=1)
         tensor_split = torch.stack(tensor_split, dim=1)
         tensor_sum = torch.sum(tensor_split, dim=2)
         tensor_size = tensor_sum.size(1)
         tensor_sorted, _ = torch.sort(tensor_sum, dim=1)
         tensor_sorted = torch.flip(tensor_sorted, [1])
         multiplier = 32. / pack
         index = int(torch.ceil((1 - mult) * tensor_size / multiplier) * multiplier)
         thres = tensor_sorted[:, index - 1].view(-1, 1)
         tensor_flag = torch.zeros_like(tensor_sum)
         tensor_flag[tensor_sum.ge(thres)] = 1
         tensor_flag[tensor_sum.le(-thres)] = 1
         tensor_flag = tensor_flag.repeat(1, pack).reshape(tensor_flag.size(0), pack, -1).transpose(2, 1).reshape_as(
            tensor_masked)

      if len(tensor_masked.size()) == 4:
         tensor_flag = tensor_flag.permute(0, 3, 1, 2)
      tensor_flag = utils_own.permute_from_list(tensor_flag, permute_list, transpose=True)
      tensor_bin = tensor.sign() * tensor_flag

   else:
      tensor_bin = tensor.sign() * mask

   return tensor_bin


class BinarizeLinear(nn.Linear):

   def __init__(self, input_bit=1, output_bit=1, *kargs, **kwargs):
      super(BinarizeLinear, self).__init__(*kargs, **kwargs)
      self.register_buffer('weight_org', self.weight.data.clone())
      self.input_bit = input_bit
      self.output_bit = output_bit

   def forward(self, input):
      if (input.size(1) != 768) and (input.size(1) != 3072):  # 784->768
         input.data = Binarize(input.data, quant_mode='multi', bitwidth=self.input_bit)
      else:
         input.data = Binarize(input.data, quant_mode='det')
      self.weight.data = Binarize(self.weight_org)
      out = nn.functional.linear(input, self.weight)
      if not self.bias is None:
         self.bias.org = self.bias.data.clone()
         out += self.bias.view(1, -1).expand_as(out)

      return out


class TernarizeLinear(nn.Linear):

   def __init__(self, thres, input_bit=1, output_bit=1, *kargs, **kwargs):
      try:
         pack = kwargs['pack']
      except:
         pack = 32
      else:
         del (kwargs['pack'])
      try:
         permute = kwargs['permute']
      except:
         permute = 1
      else:
         del (kwargs['permute'])
      try:
         self.align = kwargs['align']
      except:
         self.align = True
      else:
         del (kwargs['align'])
      super(TernarizeLinear, self).__init__(*kargs, **kwargs)
      self.input_bit = input_bit
      self.output_bit = output_bit
      permute = min(permute, self.weight.size(0))
      self.register_buffer('pack', torch.LongTensor([pack]))
      self.register_buffer('thres', torch.FloatTensor([thres]))
      self.register_buffer('mask', torch.ones_like(self.weight.data))
      self.register_buffer('permute_list', torch.LongTensor(np.tile(range(self.weight.size(1)), (permute, 1))))
      self.register_buffer('weight_org', self.weight.data.clone())

   def forward(self, input, pruned=False):
      if (input.size(1) != 768) and (input.size(1) != 3072):  # 784->768
         input.data = Binarize(input.data, quant_mode='multi', bitwidth=self.input_bit)
      else:
         input.data = Binarize(input.data, quant_mode='det')
      self.weight.data = Ternarize(self.weight_org, self.thres, self.mask, self.permute_list, pruned, align=self.align,
                                   pack=self.pack.item())
      out = nn.functional.linear(input, self.weight)
      if not self.bias is None:
         self.bias.org = self.bias.data.clone()
         out += self.bias.view(1, -1).expand_as(out)
      return out


class CustomLeakyRelu(torch.autograd.Function):

   @staticmethod
   def forward(self, input, slope, output_bitwidth):
      self.neg = input < 0
      self.slope = slope
      prev = 0
      for i in range(1, 2 ** (output_bitwidth - 1)):
         mask = torch.logical_and(input < prev, input > -i / (2 ** (output_bitwidth - 1)) / slope)
         input[mask] = -i / (2 ** (output_bitwidth))
         prev = -i / (2 ** (output_bitwidth - 1)) / slope
      mask = input < prev
      input[mask] = -1 + 1 / (2 ** output_bitwidth)
      return input

   @staticmethod
   def backward(self, grad_output):
      grad_input = grad_output.clone()
      grad_input[self.neg] *= self.slope
      return grad_input, None, None


# REF:https://github.com/cornell-zhang/dnn-gating/blob/31666fadf35789b433c79eec8669a3a2df818bd4/utils/pg_utils.py
class Floor(torch.autograd.Function):
   @staticmethod
   def forward(ctx, input):
      """
      In the forward pass we receive a Tensor containing the input and return
      a Tensor containing the output. ctx is a context object that can be used
      to stash information for backward computation. You can cache arbitrary
      objects for use in the backward pass using the ctx.save_for_backward method.
      """
      # ctx.save_for_backward(input)
      return torch.floor(input)

   @staticmethod
   def backward(ctx, grad_output):
      """
      In the backward pass we receive a Tensor containing the gradient of the loss
      with respect to the output, and we need to compute the gradient of the loss
      with respect to the input.
      The backward behavior of the floor function is defined as the identity function.
      """
      # input, = ctx.saved_tensors
      grad_input = grad_output.clone()
      return grad_input


class GreaterThan(torch.autograd.Function):
   @staticmethod
   def forward(ctx, input, threshold):
      """
      In the forward pass we receive a Tensor containing the input and return
      a Tensor containing the output. ctx is a context object that can be used
      to stash information for backward computation. You can cache arbitrary
      objects for use in the backward pass using the ctx.save_for_backward method.
      """
      return torch.Tensor.float(torch.gt(input, threshold))

   @staticmethod
   def backward(ctx, grad_output):
      """
      In the backward pass we receive a Tensor containing the gradient of the loss
      with respect to the output, and we need to compute the gradient of the loss
      with respect to the input.
      The backward behavior of the floor function is defined as the identity function.
      """
      grad_input = grad_output.clone()
      return grad_input, None


class TorchTruncate(nn.Module):
   """
   Quantize an input tensor to a b-bit fixed-point representation, and
   remain the bh most-significant bits.
       Args:
       input: Input tensor
       b:  Number of bits in the fixed-point
       bh: Number of most-significant bits remained
   """

   def __init__(self, input_bit=8, out_msb=4):
      super(TorchTruncate, self).__init__()
      self.input_bit = input_bit
      self.out_msb = out_msb

   def forward(self, input):
      # print(input)
      """ extract the sign of each element """
      sign = torch.sign(input).detach()
      """ get the mantessa bits """
      input = torch.abs(input)
      # scaling = (2.0**(self.input_bit)-1.0)/(2.0**self.input_bit) + self.epsilon
      # input = torch.clamp( input/scaling ,0.0, 1.0 )
      """ round the mantessa bits to the required precision """
      input = Floor.apply(input * (2.0 ** self.out_msb))
      """ truncate the mantessa bits """
      input = Floor.apply(input / 2.0).mul_(2).add_(1)
      """ rescale """
      input /= (2.0 ** self.out_msb)
      # print(input*sign)
      return input * sign


class BinarizeConv2d(nn.Conv2d):

   def __init__(self, input_bit=1, output_bit=1, *kargs, **kwargs):
      super(BinarizeConv2d, self).__init__(*kargs, **kwargs)
      self.register_buffer('weight_org', self.weight.data.clone())
      self.input_bit = input_bit
      self.output_bit = output_bit
      # self.threshold = nn.Parameter(torch.zeros(1,requires_grad=True))
      self.threshold = nn.Parameter(torch.zeros(input_bit, requires_grad=True))
      # self.threshold = nn.Parameter(torch.empty(input_bit-1,requires_grad=True))
      # for i in range(input_bit):
      #    self.threshold.data[i]=guess[i]
      # self.exp=True
      self.alpha = 5
      # self.trunc = TorchTruncate(input_bit,1)
      self.gt = GreaterThan.apply
      self.trunc = [None] * self.input_bit
      for i in range(self.input_bit):
         self.trunc[i] = TorchTruncate(self.input_bit, i + 1)

   def forward(self, input):
      mask = [None] * self.input_bit
      if input.size(1) != 3 and input.size(1) != 1:
         input.data = Binarize(input.data, quant_mode='multi', bitwidth=self.input_bit)
         self.weight.data = Binarize(self.weight_org)

         out_msb = nn.functional.conv2d(self.trunc[0](input),
                                        self.weight, None, self.stride, self.padding,
                                        self.dilation, self.groups)
         """ Calculate the mask """
         mask[0] = self.gt(torch.sigmoid((self.threshold[0] - out_msb)), 0.5)

         """ combine outputs """
         out = out_msb  # + mask * out_lsb
         for i in range(self.input_bit - 1):
            out_msb = nn.functional.conv2d(self.trunc[i + 1](input) - self.trunc[i](input),
                                           self.weight, None, self.stride, self.padding,
                                           self.dilation, self.groups)
            out += mask[i] * out_msb
            """ Calculate the mask """
            mask[i + 1] = self.gt(torch.sigmoid((self.threshold[i + 1] - out)), 0.5) * mask[i]
            """ perform LSB convolution """

         # out = nn.functional.conv2d(input, self.weight, None, self.stride,self.padding, self.dilation, self.groups)
      else:
         input.data = Binarize(input.data, quant_mode='input', bitwidth=self.input_bit)
         input.data = Binarize(input.data, quant_mode='multi', bitiwdth=self.input_bit)
         self.weight.data = Binarize(self.weight_org)  # , quant_mode='weight',bitwidth=8)
         out = nn.functional.conv2d(input, self.weight, None, self.stride,
                                    self.padding, self.dilation, self.groups)
      self.exp=True
      #if self.exp:
      #   now=datetime.now().time()
      #   temp=input.detach().numpy()
      #  with open ("./"+str(now.minute)+str(now.second)+str(now.microsecond)+".npy","wb") as f:
      #      np.save(f,temp)

      if not self.bias is None:
         self.bias.org = self.bias.data.clone()
         out += self.bias.view(1, -1, 1, 1).expand_as(out)
      #if self.exp:
      #    now=datetime.now().time()
      #    temp=out.detach().numpy()
      #    with open("./" + str(now.minute) + str(now.second) + str(now.microsecond) + ".npy", "wb") as f:
      #        np.save(f, temp)

      return out


class TernarizeConv2d(nn.Conv2d):

   def __init__(self, thres, input_bit=1, output_bit=1, *kargs, **kwargs):
      try:
         pack = kwargs['pack']
      except:
         pack = 32
      else:
         del (kwargs['pack'])
      try:
         permute = kwargs['permute']
      except:
         permute = 1
      else:
         del (kwargs['permute'])
      try:
         self.align = kwargs['align']
      except:
         self.align = True
      else:
         del (kwargs['align'])
      self.input_bit = input_bit
      self.output_bit = output_bit
      super(TernarizeConv2d, self).__init__(*kargs, **kwargs)
      permute = min(permute, self.weight.size(0))
      self.register_buffer('pack', torch.LongTensor([pack]))
      self.register_buffer('thres', torch.FloatTensor([thres]))
      self.register_buffer('mask', torch.ones_like(self.weight.data))
      self.register_buffer('permute_list', torch.LongTensor(np.tile(range(self.weight.size(1)), (permute, 1))))
      self.register_buffer('weight_org', self.weight.data.clone())

   def forward(self, input, pruned=False):

      if input.size(1) != 3 and input.size(1) != 1:
         input.data = Binarize(input.data, quant_mode='multi', bitwidth=self.input_bit)
      else:
         input.data = Binarize(input.data, quant_mode='input', bitwidth=self.input_bit)
      self.weight.data = Ternarize(self.weight_org, self.thres, self.mask, self.permute_list, pruned, align=self.align,
                                   pack=self.pack.item())
      # self.exp=True
      # if self.exp:
      #   now=datetime.now().time()
      #   temp=input.detach().numpy()
      #   with open ("./"+str(now.minute)+str(now.second)+str(now.microsecond)+".npy","wb") as f:
      #       np.save(f,temp)
      out = nn.functional.conv2d(input, self.weight, None, self.stride, self.padding, self.dilation, self.groups)
      if not self.bias is None:
         self.bias.org = self.bias.data.clone()
         out += self.bias.view(1, -1, 1, 1).expand_as(out)
      # if self.exp:
      #    now=datetime.now().time()
      #    temp=out.detach().numpy()
      #    with open("./" + str(now.minute) + str(now.second) + str(now.microsecond) + ".npy", "wb") as f:
      #        np.save(f, temp)
      return out

